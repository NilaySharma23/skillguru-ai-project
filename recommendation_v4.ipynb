{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugWM0lAWwKfS",
        "outputId": "fd6ca291-3e3b-4a5e-aba4-4dda4efc7bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.16.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2555924 sha256=686d22a538f3fe71ce63999aacb444f08bc6040d5d5c57f486ad7a7a6adab50e\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\n",
            "Successfully built scikit-surprise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ID8E48OPpY9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786074f4-27c6-425c-df19-97404264e24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Encoding question embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 39/39 [00:39<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2143/2143 [03:49<00:00,  9.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Hit@5': 0.010732617825478302, 'Precision@5': 0.002146523565095661, 'Recall@5': 0.010732617825478302, 'NDCG@5': np.float64(0.005697768840887685), 'MAP@5': 0.00402084305490745}\n",
            "Generating recommendations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2143/2143 [04:05<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: recommendations_v4.csv\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# RECOMMENDER v4 — Hybrid (Embeddings + Rules) — FIXED & FAST\n",
        "# Single-cell, NumPy 2.x safe\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "INTERACTIONS_F = \"model_training_data.csv\"\n",
        "QUESTION_FEAT_F = \"question_features_synthetic.csv\"\n",
        "OUT_RECS_F = \"recommendations_v4.csv\"\n",
        "\n",
        "TOP_K = 5\n",
        "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "EMB_BATCH = 256\n",
        "\n",
        "W_SIM, W_MAST, W_POP, W_REC = 0.45, 0.30, 0.15, 0.10\n",
        "\n",
        "# ---------------- LOAD DATA ----------------\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv(INTERACTIONS_F)\n",
        "qf = pd.read_csv(QUESTION_FEAT_F)\n",
        "\n",
        "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "df[\"question_id\"] = pd.to_numeric(df[\"question_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "qf[\"question_id\"] = pd.to_numeric(qf[\"question_id\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "df = df.dropna(subset=[\"student_id\",\"question_id\"])\n",
        "qf = qf.dropna(subset=[\"question_id\"])\n",
        "\n",
        "if \"synthetic_question_text\" in qf.columns:\n",
        "    qf[\"question_text\"] = qf[\"synthetic_question_text\"]\n",
        "\n",
        "qf = qf.drop_duplicates(\"question_id\").set_index(\"question_id\")\n",
        "\n",
        "# ---------------- FEATURE NORMALIZATION ----------------\n",
        "qf[\"difficulty_score\"] = qf[\"difficulty_score\"].fillna(qf[\"difficulty_score\"].median())\n",
        "qf[\"attempt_count\"] = qf[\"attempt_count\"].fillna(0)\n",
        "\n",
        "last_seen = df.groupby(\"question_id\")[\"timestamp\"].max()\n",
        "qf[\"recency_days\"] = (pd.Timestamp.now() - last_seen).dt.days.fillna(9999)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "qf[[\"difficulty_s\",\"attempt_s\",\"recency_s\"]] = scaler.fit_transform(\n",
        "    qf[[\"difficulty_score\",\"attempt_count\",\"recency_days\"]]\n",
        ")\n",
        "\n",
        "# ---------------- EMBEDDINGS ----------------\n",
        "print(\"Encoding question embeddings...\")\n",
        "model = SentenceTransformer(EMB_MODEL)\n",
        "\n",
        "q_ids = qf.index.to_numpy()\n",
        "texts = qf[\"question_text\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "embs = []\n",
        "for i in tqdm(range(0, len(texts), EMB_BATCH)):\n",
        "    embs.append(model.encode(texts[i:i+EMB_BATCH], convert_to_numpy=True))\n",
        "\n",
        "q_emb = np.vstack(embs)\n",
        "qid_to_idx = {int(q): i for i, q in enumerate(q_ids)}\n",
        "\n",
        "# ---------------- USER MASTERY TABLE ----------------\n",
        "user_skill_mastery = (\n",
        "    df.groupby([\"student_id\",\"skill\"])[\"mastery_score\"]\n",
        "    .mean()\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "user_mean = df.groupby(\"student_id\")[\"mastery_score\"].mean()\n",
        "global_mean = df[\"mastery_score\"].mean()\n",
        "\n",
        "skills_arr = qf[\"skill\"].values\n",
        "difficulty_arr = qf[\"difficulty_s\"].values\n",
        "attempt_arr = qf[\"attempt_s\"].values\n",
        "recency_arr = qf[\"recency_s\"].values\n",
        "\n",
        "# ---------------- STUDENT PROFILE ----------------\n",
        "def student_profile(uid, n=20):\n",
        "    hist = df[df.student_id == uid].sort_values(\"timestamp\", ascending=False).head(n)\n",
        "    if hist.empty:\n",
        "        return q_emb.mean(axis=0)\n",
        "\n",
        "    idxs, weights = [], []\n",
        "    for _, r in hist.iterrows():\n",
        "        idx = qid_to_idx.get(int(r.question_id))\n",
        "        if idx is not None:\n",
        "            idxs.append(idx)\n",
        "            weights.append(0.7 * r.correctness + 0.3)\n",
        "\n",
        "    vecs = q_emb[idxs]\n",
        "    w = np.array(weights)[:, None]\n",
        "    return (vecs * w).sum(axis=0) / w.sum()\n",
        "\n",
        "# ---------------- FAST VECTOR RECOMMENDER ----------------\n",
        "def recommend(uid, allow_seen=False):\n",
        "    prof = student_profile(uid)\n",
        "    sim = cosine_similarity(prof.reshape(1,-1), q_emb).flatten()\n",
        "\n",
        "    # vectorized mastery gap\n",
        "    user_mast = np.array([\n",
        "        user_skill_mastery.get((uid, s), user_mean.get(uid, global_mean))\n",
        "        for s in skills_arr\n",
        "    ])\n",
        "    mast_score = 1 - np.abs(difficulty_arr - user_mast)\n",
        "\n",
        "    score = (\n",
        "        W_SIM * sim +\n",
        "        W_MAST * mast_score +\n",
        "        W_POP * (1 - attempt_arr) +\n",
        "        W_REC * recency_arr\n",
        "    )\n",
        "\n",
        "    if not allow_seen:\n",
        "        seen = set(df[df.student_id == uid].question_id)\n",
        "        mask = np.array([qid not in seen for qid in q_ids])\n",
        "        score = np.where(mask, score, -1e9)\n",
        "\n",
        "    top_idx = np.argsort(score)[-TOP_K:][::-1]\n",
        "\n",
        "    return [{\n",
        "        \"question_id\": int(q_ids[i]),\n",
        "        \"score\": float(score[i]),\n",
        "        \"question_text\": qf.iloc[i][\"question_text\"]\n",
        "    } for i in top_idx]\n",
        "\n",
        "# ---------------- EVALUATION (FIXED) ----------------\n",
        "print(\"Evaluating...\")\n",
        "test_idx = df.groupby(\"student_id\")[\"timestamp\"].idxmax()\n",
        "test = df.loc[test_idx]\n",
        "\n",
        "hits = prec = rec = ndcg = ap = 0\n",
        "n = len(test)\n",
        "\n",
        "for _, row in tqdm(test.iterrows(), total=n):\n",
        "    recs = recommend(row.student_id, allow_seen=True)\n",
        "    ids = [r[\"question_id\"] for r in recs]\n",
        "\n",
        "    hit = int(row.question_id in ids)\n",
        "    hits += hit\n",
        "    prec += hit / TOP_K\n",
        "    rec += hit\n",
        "\n",
        "    if hit:\n",
        "        rank = ids.index(row.question_id) + 1\n",
        "        ndcg += 1 / np.log2(rank + 1)\n",
        "        ap += 1 / rank\n",
        "\n",
        "print({\n",
        "    \"Hit@5\": hits/n,\n",
        "    \"Precision@5\": prec/n,\n",
        "    \"Recall@5\": rec/n,\n",
        "    \"NDCG@5\": ndcg/n,\n",
        "    \"MAP@5\": ap/n\n",
        "})\n",
        "\n",
        "# ---------------- GENERATE FINAL RECS ----------------\n",
        "print(\"Generating recommendations...\")\n",
        "out = []\n",
        "for uid in tqdm(df.student_id.unique()):\n",
        "    out.append({\n",
        "        \"student_id\": uid,\n",
        "        \"recommendations\": recommend(uid)\n",
        "    })\n",
        "\n",
        "pd.DataFrame(out).to_csv(OUT_RECS_F, index=False)\n",
        "print(\"Saved:\", OUT_RECS_F)\n"
      ]
    }
  ]
}